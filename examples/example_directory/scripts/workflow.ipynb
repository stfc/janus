{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumed directory structure\n",
    "\n",
    "```\n",
    "example_directory\n",
    "├── active_learning\n",
    "│   ├── xyz\n",
    "│   └── simulation.lammps\n",
    "├── cp2k_input\n",
    "│   └── template.inp\n",
    "├── cp2k_output\n",
    "├── lammps\n",
    "│   └── template.lmp\n",
    "├── n2p2\n",
    "│   └── input.nn.template\n",
    "├── qe\n",
    "│   ├── pseuodos\n",
    "│   │   └── ...\n",
    "│   └── mcresol-T300-p1.xyz\n",
    "├── scripts\n",
    "│   ├── cp2k.ipynb\n",
    "│   ├── data_pruning.ipynb\n",
    "│   ├── quantum_espresso.ipynb\n",
    "│   ├── workflow.ipynb\n",
    "│   └── visualise.ipynb\n",
    "├── validation\n",
    "├── xyz\n",
    "└── reference.history\n",
    "```\n",
    "\n",
    "While functions allow for filepaths to be specified, the default arguments will assume the above directory structure, and will read and write to locations accordingly.\n",
    "\n",
    "Another aspect of how the code handles paths is the formatting of file names when creating multiple files with a regular naming pattern. For example, as only a single trajectory is expected this is given with a full file name (e.g. `'example_trajectory.history'`) but the individual frames should contain a pair of braces to allow formatting (e.g. `'xyz/{}.xyz'`).\n",
    "\n",
    "Finally, in some cases \"template\" files can be used which contain details that are not needed to be routinely changed as part of the workflow, and are not dependent on the structures being dealt with. To change these, simply modify the template files. \n",
    "\n",
    "The majority of file management and high level commands are called via the `Controller` object. This stores information about the directory structure, location of executables and the properties of the atoms in question. The latter in turn uses `Species` and `Structure` objects to store general information about the systems of interest, with specific configurations of atoms being represented by a `Dataset` and its constituent `Frames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executables, filepaths, etc.\n",
    "main_directory = '..'\n",
    "n2p2_sub_directories = ['n2p2']\n",
    "lammps_sub_directory = 'lammps'\n",
    "n2p2_bin = '/path/to/n2p2/bin'\n",
    "lammps_executable = '/path/to/lammps/build/lmp_mpi'\n",
    "n2p2_module_commands = [\n",
    "    'module use ...',\n",
    "    'module load ...',\n",
    "]\n",
    "slurm_constraint = \"constraint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cc_hdnnp.controller import Controller\n",
    "from cc_hdnnp.structure import AllStructures, Species, Structure\n",
    "\n",
    "# Create objects for all elements in the structure\n",
    "H = Species(\n",
    "    symbol='H',\n",
    "    atomic_number=1,\n",
    "    mass=1.00794,\n",
    "    valence=1,\n",
    "    min_separation={\"H\": 0.8, \"C\": 0.8, \"O\": 0.8},\n",
    ")\n",
    "C = Species(\n",
    "    symbol='C',\n",
    "    atomic_number=6,\n",
    "    mass=12.011,\n",
    "    min_separation={\"H\": 0.8, \"C\": 0.8, \"O\": 0.8},\n",
    "    valence=4,\n",
    ")\n",
    "O = Species(\n",
    "    symbol='O',\n",
    "    atomic_number=8,\n",
    "    mass=15.9994,\n",
    "    min_separation={\"H\": 0.8, \"C\": 0.8, \"O\": 0.8},\n",
    "    valence=6\n",
    ")\n",
    "\n",
    "# Define a name for the Structure which has the above constituent elements\n",
    "# Information used for active learning, such as the energy and force tolerances is also defined here\n",
    "all_species = [H, C, O]\n",
    "structure = Structure(name='mcresol', all_species=all_species, delta_E=1e-4, delta_F=1e-2)\n",
    "all_structures = AllStructures(structure)\n",
    "\n",
    "controller = Controller(\n",
    "    structures=all_structures,\n",
    "    main_directory=main_directory,\n",
    "    n2p2_sub_directories=n2p2_sub_directories,\n",
    "    lammps_sub_directory=lammps_sub_directory,\n",
    "    n2p2_bin=n2p2_bin,\n",
    "    lammps_executable=lammps_executable,\n",
    "    n2p2_module_commands=n2p2_module_commands,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate dataset\n",
    "Either [Quantum Espresso](quantum_espresso.ipynb) or [CP2K](cp2k.ipynb) can be used to generate energy, force and charge values for an input trajectory. See the individual notebooks for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. N2P2\n",
    "Once force and energy values are obtained, and written to the N2P2 data format, the rest of N2P2 can be set up prior to training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetry Functions\n",
    "Multiple different symmetry functions can be written to the same network input file, for example both shifted and centered versions of the radial, wide and narrow functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.write_n2p2_nn(\n",
    "    file_nn_template='input.nn.template',\n",
    "    file_nn='input.nn',\n",
    "    r_cutoff=12.0,\n",
    "    type='radial',\n",
    "    rule='imbalzano2018',\n",
    "    mode='center',\n",
    "    n_pairs=5\n",
    ")\n",
    "controller.write_n2p2_nn(\n",
    "    file_nn_template='input.nn.template',\n",
    "    file_nn='input.nn',\n",
    "    r_cutoff=12.0,\n",
    "    type='angular_narrow',\n",
    "    rule='imbalzano2018',\n",
    "    mode='center',\n",
    "    n_pairs=5,\n",
    "    zetas=[1]\n",
    ")\n",
    "controller.write_n2p2_nn(\n",
    "    file_nn_template='input.nn.template',\n",
    "    file_nn='input.nn',\n",
    "    r_cutoff=12.0,\n",
    "    type='angular_wide',\n",
    "    rule='imbalzano2018',\n",
    "    mode='center',\n",
    "    n_pairs=5,\n",
    "    zetas=[1]\n",
    ")\n",
    "controller.write_n2p2_nn(\n",
    "    file_nn_template='input.nn.template',\n",
    "    file_nn='input.nn',\n",
    "    r_cutoff=12.0,\n",
    "    type='radial',\n",
    "    rule='imbalzano2018',\n",
    "    mode='shift',\n",
    "    n_pairs=5\n",
    ")\n",
    "controller.write_n2p2_nn(\n",
    "    file_nn_template='input.nn.template',\n",
    "    file_nn='input.nn',\n",
    "    r_cutoff=12.0,\n",
    "    type='angular_narrow',\n",
    "    rule='imbalzano2018',\n",
    "    mode='shift',\n",
    "    n_pairs=5,\n",
    "    zetas=[1]\n",
    ")\n",
    "controller.write_n2p2_nn(\n",
    "    file_nn_template='input.nn.template',\n",
    "    file_nn='input.nn',\n",
    "    r_cutoff=12.0,\n",
    "    type='angular_wide',\n",
    "    rule='imbalzano2018',\n",
    "    mode='shift',\n",
    "    n_pairs=5,\n",
    "    zetas=[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale, normalise and prune\n",
    "Before training, the input data can optionally be normalised. This will apply headers in the relevant n2p2 files, but the other values in `input.data` will remain unchanged. Additionally, the symmetry functions must be \"scaled\", and in order to make the training process less expensive they can also be \"pruned\". Those with a low range across the `input.data` are deemed to be less desirable than those that vary a lot, and are commented out of `input.nn`.\n",
    "\n",
    "Both the script for these pre-training steps andthe training itself are generated from one function taking many optional arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.write_n2p2_scripts(range_threshold=1e-4, ntasks_per_node=1, constraint=slurm_constraint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preparation required before training can then be run as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sbatch n2p2_prepare.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network\n",
    "Provided there are an acceptable number of symmetry functions after pruning (if not re-run with a higher or lower threshold) the network can now be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sbatch n2p2_train.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights selection\n",
    "Once training is finished (either by completing all epochs or reaching the time limit) a set of weights should be chosen. Either a specific epoch can be chosen, or an epoch can be automatically chosen in order to minimise one of the errors calculated as a metric during the training process. If both `epoch` and `minimum_criterion` are `None` then the most recent epoch will be chosen by default.\n",
    "\n",
    "Note that since multiple networks are required for the active learning workflows, the index of the directory in question should also be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = None\n",
    "minimum_criterion = None\n",
    "# minimum_criterion = \"RMSEpa_Etest_pu\"\n",
    "# minimum_criterion = \"MAEpa_Etest_pu\"\n",
    "# minimum_criterion = \"RMSE_Ftest_pu\"\n",
    "# minimum_criterion = \"MAE_Ftest_pu\"\n",
    "file_out = \"weights.{0:03d}.data\"\n",
    "controller.choose_weights(\n",
    "    n2p2_directory_index=0,\n",
    "    epoch=epoch,\n",
    "    minimum_criterion=minimum_criterion,\n",
    "    file_out=file_out,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LAMMPS Validation\n",
    "\n",
    "Once the network is trained it can be used in LAMMPS to run MD simulations. An existing `.xyz` file can be used with the `write_lammps_data` function, or a `Dataset` object can be written in the `\"lammps-data\"` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.write_lammps_data(file_xyz='xyz/0.xyz', lammps_unit_style='metal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cc_hdnnp.dataset import Dataset\n",
    "dataset = Dataset(data_file=f\"{main_directory}/{n2p2_sub_directories[0]}/input.data\")\n",
    "dataset.write(\n",
    "    file_out=f\"{main_directory}/{lammps_sub_directory}/lammps.data\",\n",
    "    format=\"lammps-data\",\n",
    "    conditions=(i == 0 for i in range(len(dataset))),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrapolations\n",
    "N2P2 automatically produces warnings when the network is extrapolating out of the range it was trained with, and will abort the MD if enough are produced. To see how many of these are produced in different conditions, scripts for a range of ensembles and temperatures can be produced, run, and analysed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.write_extrapolations_lammps_script(\n",
    "    n2p2_directory_index=0, temperatures=range(290, 310), constraint=slurm_constraint,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sbatch lammps_extrapolations.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.analyse_extrapolations(temperatures=range(290, 310))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDF Validation\n",
    "The dumps files generated by the extrapolations tests (or otherwise) can also have their RDF compared to that of the original trajectory used in dataset generation. This first requires conversion into pdb and xyz formats for use with the external aml package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.io import write, read\n",
    "from aml.score.rdf import run_rdf_test\n",
    "from aml.score import load_with_cell\n",
    "\n",
    "filepath_ref = \"reference.history\"\n",
    "filepath_net = f\"../{lammps_sub_directory}/nve_t290_xyz_dump.lammpstrj\"\n",
    "\n",
    "controller.read_trajectory(filepath_ref)\n",
    "write(\"../validation/ref_pos.xyz\", controller.trajectory)\n",
    "write(\"../validation/ref.pdb\", controller.trajectory)\n",
    "\n",
    "lammps_dump = read(filepath_net, format=\"lammps-dump-text\", index=\":\")\n",
    "write(\"../validation/net_pos.xyz\", lammps_dump)\n",
    "write(\"../validation/net.pdb\", lammps_dump)\n",
    "\n",
    "traj = load_with_cell(\"../validation/ref_pos.xyz\", top=\"../validation/ref.pdb\")\n",
    "traj_net = load_with_cell(\"../validation/net_pos.xyz\", top=\"../validation/net.pdb\")\n",
    "run_rdf_test(traj, traj_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Active Learning\n",
    "It is likely that the initial reference structures/energies used for training do not fully describe the system. By training a second network on the same data, active learning can be used to extend the reference structures and energies in regions where the two networks do not agree. Assuming there are two such networks in directories in `../n2p2_1` and `../n2p2_2`, the first step is to generate the necessary LAMMPS input files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cc_hdnnp.active_learning import ActiveLearning\n",
    "\n",
    "active_learning_sub_directory = 'active_learning'\n",
    "\n",
    "al_controller = Controller(\n",
    "    structures=all_structures,\n",
    "    main_directory=main_directory,\n",
    "    n2p2_bin=n2p2_bin,\n",
    "    lammps_executable=lammps_executable,\n",
    "    n2p2_sub_directories=[\"n2p2_1\", \"n2p2_2\"],\n",
    "    n2p2_module_commands=n2p2_module_commands,\n",
    "    active_learning_sub_directory=active_learning_sub_directory,\n",
    ")\n",
    "\n",
    "a = ActiveLearning(data_controller=al_controller)\n",
    "a.write_lammps(temperatures=[300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run LAMMPS using the appropriate batch script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sbatch active_learning_lammps.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trajectories generated by LAMMPS are pre-analysed and where appropriate reduced, before writing the new configurations to be considered to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.prepare_lammps_trajectory()\n",
    "a.prepare_data_new(constraint=slurm_constraint, ntasks_per_node=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the NNs using the appropriate batch script to evaluate the energies for this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sbatch active_learning_nn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the energy evaluations of the NNs, the configurations to add to the training set can be determined by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.prepare_data_add()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates the file `input.data-add` in the active learning directory, however we still need to generate reference energies (as we have so far only evaluated the NN). This is done in the same manner as in section 1, but first requires converting into the xyz format (the exact method will depend on whether [Quantum Espresso](quantum_espresso.ipynb) or [CP2K](cp2k.ipynb) was used).\n",
    "\n",
    "For CP2K this should result in multiple files, with each filename containing a frame index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al_controller.convert_active_learning_to_xyz('input.data-add', f\"{active_learning_sub_directory}/xyz/{{}}.xyz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For QE, a single file containing the name of the `Structure` and indications of the temperature and pressure should be included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al_controller.convert_active_learning_to_xyz(\n",
    "    file_n2p2_data='input.data-add',\n",
    "    file_xyz=f\"{active_learning_sub_directory}/xyz/mcresol-T300-p1.xyz\",\n",
    "    single_output=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the `input.data` file already exists, then this will append the active learning structures to the existing file. Then the training can be restarted with a wider selection of data to ensure a more applicable model. However, it is worth noting that the scaling/normalisation process will need to be re-done. To remove the outdated normalisation header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al_controller.remove_n2p2_normalisation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Manipulation\n",
    "Following the active learning, it may be that the increased dataset is no longer practical to use due to some outlying values or simply by being too large to fit into memory. There are a few different methods of reducing its size.\n",
    "\n",
    "Firstly, structures with neighbouring atoms within a specified minimum seperation can be removed. This is done during the active learning process, but can also be done after the fact if a higher threshold is desired: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cc_hdnnp.dataset import Dataset\n",
    "\n",
    "for species in structure.all_species:\n",
    "    species.min_separation = {\"H\": 0.9, \"C\": 0.9, \"O\": 0.9}\n",
    "\n",
    "dataset = Dataset(\n",
    "    data_file=f\"../{active_learning_sub_directory}/mode2/HDNNP_1/input.data\",\n",
    "    all_structures=AllStructures(structure),\n",
    ")\n",
    "dataset.write(\n",
    "    file_out=f\"../{active_learning_sub_directory}/mode2/HDNNP_1/input.data.nearest_neighbours\",\n",
    "    conditions=dataset.check_min_separation_all()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, a threshold in energy and/or force values can be set. Care should be taken over the units used here: both `energy_threshold` and `force_threshold` should be in the same units as those expressed in the `Dataset`. Also, either a single float or a tuple of floats can be given for `energy_threshold`. The former is taken as `(-energy_threshold, energy_threshold)` and so is only suitable when using normalised units with a mean of 0. As forces are always expected to have a symmetric distribution about zero, only a single float is supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.write(\n",
    "    file_out=f\"../{active_learning_sub_directory}/mode2/HDNNP_1/input.data.outliers\",\n",
    "    conditions=dataset.check_threshold_all(\n",
    "        energy_threshold=(-1150, -1100), force_threshold=1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complicated methods of pruning the dataset can be found in [Data Pruning](data_pruning.ipynb)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1f3a24fad89bf6a26a1a265e74673afc0a05777de7be9df46ce49311c93c097"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
