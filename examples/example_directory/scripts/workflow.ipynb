{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumed directory structure\n",
    "\n",
    "```\n",
    "example_directory\n",
    "├── active_learning\n",
    "│   └── simulation.lammps\n",
    "├── cp2k_input\n",
    "│   └── example_template.inp\n",
    "├── cp2k_output\n",
    "├── lammps\n",
    "│   ├── md-nve.lmp\n",
    "│   ├── md-nvt.lmp\n",
    "│   ├── md-npt.lmp\n",
    "│   └── template.lmp\n",
    "├── n2p2\n",
    "│   └── input.nn.template\n",
    "├── scripts\n",
    "│   ├── cp2k.ipynb\n",
    "│   ├── quantum_espresso.ipynb\n",
    "│   ├── workflow.ipynb\n",
    "│   ├── visualise.ipynb\n",
    "│   └── template.sh\n",
    "├── xyz\n",
    "└── example_trajectory.history\n",
    "```\n",
    "\n",
    "While functions allow for filepaths to be specified, the default arguments will assume the above directory structure, and will read and write to locations accordingly.\n",
    "\n",
    "Another aspect of how the code handles paths is the formatting of file names when creating multiple files with a regular naming pattern. For example, as only a single trajectory is expected this is given with a full file name (e.g. `'example_trajectory.history'`) but the individual frames should contain a pair of braces to allow formatting (e.g. `'xyz/{}.xyz'`).\n",
    "\n",
    "Finally, there is a reliance on \"template\" files which contain details that are not needed to be configured between different frames etc. To change these, simply modify the template files. These contain details that should not change drastically for different experiments (such as settings for the Slurm scheduler) but may need altering due to memory constraints (for example).\n",
    "\n",
    "The majority of file management commands are called via the `Data` object. This stores information about the directory structure, location of executables and the properties of the atoms in question. The latter in turn uses `Species` and `Structure` objects to store information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cc_hdnnp.data import Data\n",
    "from cc_hdnnp.structure import AllSpecies, AllStructures, Species, Structure\n",
    "\n",
    "# Create objects for all elements in the structure\n",
    "H = Species(symbol='H', atomic_number=1, mass=1.00794, min_separation={\"H\": 0.8, \"C\": 0.8, \"O\": 0.8})\n",
    "C = Species(symbol='C', atomic_number=6, mass=12.011, min_separation={\"H\": 0.8, \"C\": 0.8, \"O\": 0.8})\n",
    "O = Species(symbol='O', atomic_number=8, mass=15.9994, min_separation={\"H\": 0.8, \"C\": 0.8, \"O\": 0.8})\n",
    "\n",
    "# Define a name for the Structure which has the above constituent elements\n",
    "# Information used for active learning, such as the energy and force tolerances is also defined here\n",
    "all_species = AllSpecies(H, C, O)\n",
    "structure = Structure(name='mcresol', all_species=all_species, delta_E=1e-4, delta_F=1e-2)\n",
    "all_structures = AllStructures(structure)\n",
    "\n",
    "# Directories and constant file locations\n",
    "main_directory = '..'\n",
    "n2p2_bin = '/path/to/n2p2/bin'\n",
    "lammps_executable = '/path/to/lammps/build/lmp_mpi'\n",
    "basis_set = \"/path/to/cp2k/data/BASIS_MOLOPT\"\n",
    "potential = \"/path/to/cp2k/data/GTH_POTENTIALS\"\n",
    "\n",
    "d = Data(\n",
    "    structures=all_structures,\n",
    "    main_directory=main_directory,\n",
    "    n2p2_bin=n2p2_bin,\n",
    "    lammps_executable=lammps_executable\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate dataset\n",
    "Either [Quantum Espresso](quantum_espresso.ipynb) or [CP2K](cp2k.ipynb) can be used to generate energy, force and charge values for an input trajectory. See the individual notebooks for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. N2P2\n",
    "Once force and energy values are obtained, and written to the N2P2 data format, the rest of N2P2 can be set up prior to training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetry Functions\n",
    "Multiple different symmetry functions can be written to the same network input file, for example both shifted and centered versions of the radial, wide and narrow functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.write_n2p2_nn(file_template='input.nn.template',\n",
    "                file_nn='input.nn',\n",
    "                r_cutoff=12.0,\n",
    "                type='radial',\n",
    "                rule='imbalzano2018',\n",
    "                mode='center',\n",
    "                n_pairs=5)\n",
    "d.write_n2p2_nn(file_template='input.nn.template',\n",
    "                file_nn='input.nn',\n",
    "                r_cutoff=12.0,\n",
    "                type='angular_narrow',\n",
    "                rule='imbalzano2018',\n",
    "                mode='center',\n",
    "                n_pairs=5,\n",
    "                zetas=[1])\n",
    "d.write_n2p2_nn(file_template='input.nn.template',\n",
    "                file_nn='input.nn',\n",
    "                r_cutoff=12.0,\n",
    "                type='angular_wide',\n",
    "                rule='imbalzano2018',\n",
    "                mode='center',\n",
    "                n_pairs=5,\n",
    "                zetas=[1])\n",
    "d.write_n2p2_nn(file_template='input.nn.template',\n",
    "                file_nn='input.nn',\n",
    "                r_cutoff=12.0,\n",
    "                type='radial',\n",
    "                rule='imbalzano2018',\n",
    "                mode='shift',\n",
    "                n_pairs=5)\n",
    "d.write_n2p2_nn(file_template='input.nn.template',\n",
    "                file_nn='input.nn',\n",
    "                r_cutoff=12.0,\n",
    "                type='angular_narrow',\n",
    "                rule='imbalzano2018',\n",
    "                mode='shift',\n",
    "                n_pairs=5,\n",
    "                zetas=[1])\n",
    "d.write_n2p2_nn(file_template='input.nn.template',\n",
    "                file_nn='input.nn',\n",
    "                r_cutoff=12.0,\n",
    "                type='angular_wide',\n",
    "                rule='imbalzano2018',\n",
    "                mode='shift',\n",
    "                n_pairs=5,\n",
    "                zetas=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale, normalise and prune\n",
    "Before training, the input data can optionally be normalised. This will apply headers in the relevant n2p2 files, but the other values in `input.data` will remain unchanged. Additionally, the symmetry functions must be \"scaled\", and in order to make the training process less expensive they can also be \"pruned\". Those with a low range across the `input.data` are deemed to be less desirable than those that vary a lot, and are commented out of `input.nn`.\n",
    "\n",
    "Both the script for these pre-training steps andthe training itself are generated from one function taking many optional arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.write_n2p2_scripts(range_threshold=1e-4)\n",
    "!sbatch ../scripts/n2p2_prepare.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network\n",
    "Provided there are an acceptable number of symmetry functions after pruning (if not re-run with a higher or lower threshold) the network can now be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sbatch ../scripts/n2p2_train.bat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most recent weights (those from the last epoch) are copied and renamed to the format `weights.<atomic_number>.data` if all epochs finish. If for whatever reason a different epoch is desired, then the files should be renamed manually.\n",
    "\n",
    "### Weights selection\n",
    "Selecting a specific epoch's weights can be done, or an epoch can be automatically chosen in order to minimise one of the errors calculated as a metric during the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = None\n",
    "# minimum_criterion = None\n",
    "# minimum_criterion = \"RMSEpa_Etest_pu\"\n",
    "# minimum_criterion = \"MAEpa_Etest_pu\"\n",
    "minimum_criterion = \"RMSE_Ftest_pu\"\n",
    "# minimum_criterion = \"MAE_Ftest_pu\"\n",
    "file_out = \"weights.{0:03d}.data\"\n",
    "if epoch:\n",
    "    file_out += \"_chosen_\" + str(epoch)\n",
    "elif minimum_criterion:\n",
    "    file_out += \"_chosen_\" + str(minimum_criterion)\n",
    "d.choose_weights(\n",
    "    n2p2_directory_index=0, epoch=epoch, minimum_criterion=minimum_criterion, file_out=file_out\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LAMMPS Validation\n",
    "\n",
    "Once the network is trained it can be used in LAMMPS to run MD simulations. A general script can be formatted from an existing `.xyz` file, the `write_lammps_data` functon can be used. The interaction is defined by `write_lammps_pair`, which creates a LAMMPS input file based on the template provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.write_lammps_data(file_xyz='xyz/0.xyz', lammps_unit_style='metal')\n",
    "d.write_lammps_pair(\n",
    "    structure=structure,\n",
    "    n_steps=1000,\n",
    "    r_cutoff=6.351,\n",
    "    file_template='lammps/template.lmp',\n",
    "    file_out='lammps/md.lmp',\n",
    "    n2p2_directory='n2p2',\n",
    "    lammps_unit_style='metal',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrapolations\n",
    "N2P2 automatically produces warnings when the network is extrapolating out of the range it was trained with, and will abort the MD if enough are produced. To see how many of these are produced in different conditions, scripts for a range of ensembles and temperatures can be produced, run, and analysed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.write_extrapolations_lammps_script(\n",
    "    n2p2_directory=d.n2p2_directories[0], temperatures=range(290, 310),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sbatch lammps_extrapolations.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.analyse_extrapolations(temperatures=range(290, 310))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDF Validation\n",
    "The dumps files generated by the extrapolations tests (or otherwise) can also have their RDF compared to that of the original trajectory used in dataset generation. This first requires conversion into pdb and xyz formats for use with the external aml package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.io import write, read\n",
    "from aml.score.rdf import run_rdf_test\n",
    "from aml.score import load_with_cell\n",
    "\n",
    "filepath_ref = \"reference.history\"\n",
    "filepath_net = \"../lammps/custom_nve.dump\"\n",
    "\n",
    "d.read_trajectory(filepath_ref)\n",
    "write(\"../validation/ref_pos.xyz\", d.trajectory)\n",
    "write(\"../validation/ref.pdb\", d.trajectory)\n",
    "\n",
    "lammps_dump = read(filepath_net, format=\"lammps-dump-text\", index=\":\")\n",
    "write(\"../validation/net_pos.xyz\", lammps_dump)\n",
    "write(\"../validation/net.pdb\", lammps_dump)\n",
    "\n",
    "traj = load_with_cell(\"../validation/ref_pos.xyz\", top=\"../validation/ref.pdb\")\n",
    "traj_net = load_with_cell(\"../validation/net_pos.xyz\", top=\"../validation/net.pdb\")\n",
    "run_rdf_test(traj, traj_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Active Learning\n",
    "It is likely that the initial reference structures/energies used for training do not fully describe the system. By training a second network on the same data, active learning can be used to extend the reference structures and energies in regions where the two networks do not agree. Assuming there are two such networks in directories in `../n2p2_1` and `../n2p2_2`, the first step is to generate the necessary LAMMPS input files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_learning import ActiveLearning\n",
    "\n",
    "d2 = Data(\n",
    "    structures=all_structures,\n",
    "    main_directory=main_directory,\n",
    "    n2p2_bin=n2p2_bin,\n",
    "    lammps_executable=lammps_executable,\n",
    "    n2p2_directories=[\"n2p2_1\", \"n2p2_2\"]\n",
    ")\n",
    "\n",
    "a = ActiveLearning(data_controller=d2)\n",
    "a.write_lammps(range(325, 375))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run LAMMPS using the appropriate batch script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sbatch ../scripts/active_learning_lammps.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trajectories generated by LAMMPS are pre-analysed and where appropriate reduced, before writing the new configurations to be considered to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.prepare_lammps_trajectory()\n",
    "a.prepare_data_new()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the NNs using the appropriate batch script to evaluate the energies for this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sbatch ../scripts/active_learning_nn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the energy evaluations of the NNs, the configurations to add to the training set can be determined by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.prepare_data_add()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates the file `input.data-add` in the active learning directory, however we still need to generate reference energies (as we have so far only evaluated the NN). This is done in the same manner as in section 1, but first requires converting into the xyz format (the exact method will depend on whether [Quantum Espresso](quantum_espresso.ipynb) or [CP2K](cp2k.ipynb) was used).\n",
    "\n",
    "For CP2K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.convert_active_learning_to_xyz('input.data-add', 'active_learning/xyz/{}.xyz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For QE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valences = {'O': 6, 'Zr': 12, 'C': 4, 'H': 1}\n",
    "d2.n2p2_directory = \"../n2p2_1\"\n",
    "d2.write_n2p2_data_qe(\n",
    "    structure_name=\"UiO-66Zr\", temperatures=[300], pressures=[1], valences=valences,\n",
    ")\n",
    "d2.n2p2_directory = \"../n2p2_2\"\n",
    "d2.write_n2p2_data_qe(\n",
    "    structure_name=\"UiO-66Zr\", temperatures=[300], pressures=[1], valences=valences,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the `input.data` file already exists, then this will append the active learning structures to the existing file. Then the training can be restarted with a wider selection of data to ensure a more applicable model. However, it is worth noting that the scaling/normalisation process will need to be re-done. To remove the outdated normalisation header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.remove_n2p2_normalisation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Manipulation\n",
    "Following the active learning, it may be that the increased dataset is no longer practical to use due to some outlying values or simply by being too large to fit into memory. There are a few different methods of reducing its size.\n",
    "\n",
    "Firstly, structures with neighbouring atoms within a specified minimum seperation can be removed. This is done during the active learning process, but can also be done after the fact if a higher threshold is desired: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for species in structure.all_species.species_list:\n",
    "    species.min_separation = {\"H\": 0.9, \"C\": 0.9, \"O\": 0.9}\n",
    "\n",
    "d.trim_dataset_separation(\n",
    "    structure=structure,\n",
    "    data_file_in=\"input.data\",\n",
    "    data_file_out=\"input.data\",\n",
    "    data_file_backup=\"input.data.minimum_separation_backup\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, a threshold in energy and/or force values can be set. Care should be taken over the units used here: both `energy_threshold` and `force_threshold` should be in the same units as those expressed in the `reference_file`. By default this is `output.data` which uses network internalised units (mean 0, standard deviation 1) but if `input.data` is used then these should be in physical units. Also, either a single float or a tuple of floats can be given for `energy_threshold`. The former is taken as `(-energy_threshold, energy_threshold)` and so is only suitable when using normalised units with a mean of 0. As forces are always expected to have a mean of (approximately) zero, only a single float is suppported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.remove_outliers(\n",
    "    energy_threshold=(-2, 2),\n",
    "    force_threshold=10,\n",
    "    data_file_in=\"input.data\",\n",
    "    data_file_out=\"input.data\",\n",
    "    data_file_backup=\"input.data.outliers_backup\",\n",
    "    reference_file=\"output.data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the dataset can be rebuilt by selecting the structures which have the greatest separation between their fingerprint vectors. Initially, structures can be specified, randomly selected, or those which contain the maximal/minimal values chosen to form the start of the new dataset. Then batches of new structures are proposed and their symmetry function vectors compared for all constituent atoms against those already selected. Those with the greatest difference are selected, and the process continues for the rest of the original dataset.\n",
    "\n",
    "Note that comparing the symmetry functions required far more memory and compute than the previous two methods, so is best run as a batch script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.rebuild_dataset(\n",
    "    atoms_per_frame=512,\n",
    "    n_frames_to_select=5,\n",
    "    n_frames_to_propose=10,\n",
    "    n_frames_to_compare=100,\n",
    "    select_extreme_frames=True,\n",
    "    starting_frame_indices=None,\n",
    "    criteria=\"mean\",\n",
    "    n2p2_directory=d.n2p2_directories[0], \n",
    "    data_file_in=\"input.data\",\n",
    "    data_file_out=\"input.data\",\n",
    "    data_file_backup=\"input.data.rebuild_backup\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "014a019344473c251a50cb45dcadf8d9c61fae40c5010d2c3a9b5a9dfbb2eb38"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('cc-hdnnp-Ufmq_e4K-py3.8': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
