{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pruning\n",
    "\n",
    "As the active learning progresses, the dataset of reference atomic configurations can grow prohibitively large, especially if a large number of symmetry functions are used. There are a number of methods available to reduce the amount of data whilst attempting to retain representation of the structure in question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumed directory structure\n",
    "\n",
    "```\n",
    "example_directory\n",
    "├── n2p2\n",
    "│   ├── atomic-env.G.data\n",
    "│   ├── input.data\n",
    "│   ├── input.nn\n",
    "│   └── scaling.data\n",
    "└── scripts\n",
    "    ├── cp2k.ipynb\n",
    "    ├── data_pruning.ipynb\n",
    "    ├── quantum_espresso.ipynb\n",
    "    ├── workflow.ipynb\n",
    "    └── template.sh\n",
    "```\n",
    "\n",
    "In order to run data selection, `\"input.data\"`, `\"input.nn\"` and `\"atomic-env.G\"` must be present. The latter can be generated by running `nnp-atomenv`, be care should be taken as for large data files and large numbers of symmetry functions, this can generate very large files that can then be difficult to hold in memory.\n",
    "\n",
    "While the network need not be trained prior to pruning the data, it is advisable to run pruning (e.g. `nnp-prune range 1.0E-4`) beforehand, to ensure that symmetry functions with negligible values are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executables and filepaths\n",
    "main_directory = '..'\n",
    "# n2p2_bin = '/path/to/n2p2/bin'\n",
    "n2p2_bin = '/home/vol00/scarf860/cc_placement/n2p2/bin'\n",
    "# lammps_executable = '/path/to/lammps/build/lmp_mpi'\n",
    "lammps_executable = '/home/vol00/scarf860/cc_placement/lammps/build/lmp_mpi'\n",
    "# qe_module_commands = [\n",
    "#     'module use ...',\n",
    "#     'module load ...',\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from cc_hdnnp.controller import Controller\n",
    "from cc_hdnnp.structure import AllStructures, Species, Structure\n",
    "import cc_hdnnp.visualisation as vis\n",
    "\n",
    "# Create objects for all elements in the structure\n",
    "H = Species(\n",
    "    symbol='H',\n",
    "    atomic_number=1,\n",
    "    mass=1.00794,\n",
    "    valence=1,\n",
    "    min_separation={\"H\": 0.8, \"C\": 0.8, \"O\": 0.8},\n",
    ")\n",
    "C = Species(\n",
    "    symbol='C',\n",
    "    atomic_number=6,\n",
    "    mass=12.011,\n",
    "    min_separation={\"H\": 0.8, \"C\": 0.8, \"O\": 0.8},\n",
    "    valence=4,\n",
    ")\n",
    "O = Species(\n",
    "    symbol='O',\n",
    "    atomic_number=8,\n",
    "    mass=15.9994,\n",
    "    min_separation={\"H\": 0.8, \"C\": 0.8, \"O\": 0.8},\n",
    "    valence=6\n",
    ")\n",
    "\n",
    "# Define a name for the Structure which has the above constituent elements\n",
    "# Information used for active learning, such as the energy and force tolerances is also defined here\n",
    "all_species = [H, C, O]\n",
    "structure = Structure(name='mcresol', all_species=all_species, delta_E=1e-4, delta_F=1e-2)\n",
    "all_structures = AllStructures(structure)\n",
    "\n",
    "controller = Controller(\n",
    "    structures=all_structures,\n",
    "    main_directory=main_directory,\n",
    "    n2p2_bin=n2p2_bin,\n",
    "    lammps_executable=lammps_executable\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CUR decomposition\n",
    "\n",
    "As described by [Imbalzano et al. (2018)](https://arxiv.org/abs/1804.02150), this method of feature selection results in features that were present in the original dataset, as opposed to the features normally returned by SVD which could not then be used in the machine learning workflow. Specifically, the features returned need to be a symmetry function or single frame of the `\"input.data\"` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cc_hdnnp.data_selection import Decomposer\n",
    "\n",
    "decomposer = Decomposer(data_controller=controller)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetry functions\n",
    "The first mode of operation reduces the number of symmetry functions present in `\"input.nn\"` from some intentionally large set. This set can be generated in the usual manner (see [Workflow](workflow.ipynb)) either using Imbalzano's initial criteria or another method. A key feature of this approach is the weighting of symmetry functions, so that multiple functions that are rarely evaluated (i.e. with low cutoff and density of contributing atoms) may be selected in favour of one that is evaluated commonly, even if the latter has a greater contribution. This can be controlled with the `weight` argument.\n",
    "\n",
    "Furthermore, multiple sets of symmetry functions can be output from a single run by providing a list to `n_to_select_list` and `file_out_list`. This can be useful to determine an appropriate trade off between model accuracy and time taken to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposer.run_CUR_symf(\n",
    "    n_to_select_list=[64, 128, 256],\n",
    "    weight=True,\n",
    "    file_out_list=[\"input.nn.64\", \"input.nn.128\", \"input.nn.256\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datapoints\n",
    "\n",
    "Alternatively, the number of datapoints in the `\"input.data\"` file can be reduced. Here, a datapoint is a whole atomic configuration representing the structure, so will include several atomic positions. The evaluations performed in `\"atomic-env.G\"` are done for each atom and each symmetry function relevant to its element. This means before selection, the vector of symmetry functions for each element present are averaged across the structure, and concatenated to give a a representation for each structure that has a length of `N_A + N_B + N_C ...` where there are `N_A` symmetry functions for element `A` and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposer.run_CUR_data(\n",
    "    n_to_select_list=[64, 128, 256],\n",
    "    file_out_list=[\"input.data.64\", \"input.data.128\", \"input.data.256\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Separator\n",
    "An alternative method for reducing the size of `\"input.data\"` is to remove atomic configurations that have a small Euclidean distance between them in terms of the vector of symmetry functions for all their atoms.\n",
    "\n",
    "As this results in comparing a large parameter space (`n_frames_to_propose * n_frames_to_compare * n_atoms ** 2 * n_symf`), it can be necessary to compare the points in batches, with the proposed frames with greatest distance selected in favour of those that are closer to frames already selected. By default, it is the mean distance that is used as a criteria, but a float corresponding to a quantile can also be used (e.g. `0.5` for the median distance).\n",
    "\n",
    "Finally, to avoid removing the most extreme points in the dataset (and so increasing the number of extrapolation warnings) `select_extreme_frames` can be set. This starts by selecting datapoints that give rise to the most extreme values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cc_hdnnp.data_selection import Separator\n",
    "\n",
    "separator = Separator(data_controller=controller)\n",
    "separator.run_separation_selection(\n",
    "    n_frames_to_select=8, n_frames_to_propose=16, n_frames_to_compare=16, select_extreme_frames=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clustering\n",
    "\n",
    "While not strictly a method for reducing the data, clustering similar atomic environments can be useful for visualisation. Clustering can be performed on atomic environments, per element, so visualise the different environments they inhabit. Alternatively, the global frame environment can be compared to see how similar different frames in the dataset are, particularly if data is added in batches during the active learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cc_hdnnp.data_selection import Clusterer\n",
    "\n",
    "clusterer = Clusterer(data_controller=controller)\n",
    "clusterer.run_atom_clustering()\n",
    "clusterer.run_frame_clustering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_clustering(elements=[\"H\", \"C\", \"O\"])\n",
    "vis.plot_clustering(elements=[\"all\"])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1f3a24fad89bf6a26a1a265e74673afc0a05777de7be9df46ce49311c93c097"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('cc-hdnnp-Ufmq_e4K-py3.8': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
